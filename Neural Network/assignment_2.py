# -*- coding: utf-8 -*-
"""Assignment_2.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1Uyzn-f8BkEIEmjp168tkndy0iuH1M2WH

# Assignment 2 - Machine Learning Practical
Due Date: Tuesday April 2nd @ 11:59 PM on blackboard

The goal of this assignment is to experiment with your previous Linear Regression and Logistic regression models to explore some of the practical skills previously covered in the course:
- Learning curves
- Regularization
- Model selection

For this assignment, you are given some boilerplate code in the form of a jupyter notebook. Your task is to fill in the blanks.
"""

# %matplotlib inline

import matplotlib.pyplot as plt
import numpy as np
import sklearn.preprocessing
import sklearn.model_selection


import keras
from keras.models import Sequential
from keras.layers import Dense
from keras.regularizers import l2
from keras.optimizers import SGD

def logistic_regression(num_inputs, regularization, learning_rate):
  model = Sequential()
  model.add(Dense(1, input_shape=(num_inputs,), activation='sigmoid', kernel_regularizer=l2(regularization)))
  model.compile(optimizer=SGD(learning_rate), loss='binary_crossentropy', metrics=['acc', keras.metrics.binary_crossentropy])
  return model

# This cell loads the data
import pandas as pd
import numpy as np
import sklearn.datasets
dataset = sklearn.datasets.load_breast_cancer()
X = dataset['data']
y = dataset['target']
print(X.shape)
print(y.shape)

# This cell visualizes the first two features of the dataset. There are actually 30 features.
f, ax = plt.subplots(1,1, figsize=(10, 10))
malignant = X[y == 0]
benign = X[y == 1]
ax.scatter(malignant[:, 0], malignant[:, 1], marker='x', s=80, label='malignant')
ax.scatter(benign[:, 0], benign[:, 1], marker='o', s=80, label='benign')
ax.set_title("Visualizing Dataset using first two features")
ax.legend();

# This code creates a logistic regression model to train on the first two inputs features, with 0 regularization, and a learning rate of 0.0001
# See the keras documentation for how to use model.fit
# https://keras.io/models/model/#fit

model = logistic_regression(num_inputs=2, regularization=0, learning_rate=0.0001)
history = model.fit(X[:, [0,1]], y, epochs=5, batch_size=16, validation_split=0.2)

# The fit function returns a 'history' object that contains the metrics tracked during training

print('validation accuracy', history.history['val_acc'])
print('training accuracy', history.history['acc'])
print("These are the different metrics you can extract from the history object:", history.history.keys())

# Create polynomial features
poly = sklearn.preprocessing.PolynomialFeatures(degree=2)
X_poly = poly.fit_transform(X)

# Split into training and validation data
X_train, X_val, y_train, y_val = sklearn.model_selection.train_test_split(X_poly, y)

# You should use X_train, y_train and X_val, y_val from now on, for your training set/validation sets respectively
X_val.shape

# Normalize your data IN THIS CELL by subtracting the mean and dividing by the standard deviation.
# Normalize it in place (so use the same variable names X_train, X_val, etc)

# TODO: Done 
# Reference for logic 
# https://stats.stackexchange.com/questions/174823/how-to-apply-standardization-normalization-to-train-and-testset-if-prediction-i
# As per the discussion we need to use the same mean and standard deviation for traning and test/validation data as they have same statistics


mean_X = np.mean(X_train)
standard_deviation_X = np.std(X_train)

mean_Y = np.mean(y_train)
standard_deviation_Y = np.std(y_train)

X_train = (X_train - mean_X)/standard_deviation_X
X_val = (X_val - mean_X)/standard_deviation_X


y_train = (y_train - mean_Y)/standard_deviation_Y
y_val = (y_val - mean_Y)/standard_deviation_Y

# Train 5 logistic regression models in a for loop on this data with, with 0 regularization, and vary the value of the 
# learning rate using the values 1.0, 0.1, 0.001, 0.0001, 0.000001
# Use a batch size of 32
# Train for 50 epochs
# plot the validation loss for all 5 models on one plot, and the validation accuracy for all 5 models on a separate plot
# Modify the code IN THIS CELL for this task, you should replace the repeated code with a for loop

# Plot 5 curves on the same plot
f, ax = plt.subplots(1, 2, figsize=(20, 10))

ax[0].set_title("Validation loss")

learning_rate = [1.0, 0.1, 0.001, 0.0001, 0.000001]
for x in learning_rate:
  model = logistic_regression(num_inputs=2, regularization=0, learning_rate= x)
  history = model.fit(X_train[:, [0,1]], y_train, epochs=50, batch_size=32, validation_split=0.2)
  if x == 1.0:
    ax[0].plot(history.history['val_loss'], 'b', label=r'$\alpha = 1.0$')  
  elif x == 0.1:
    ax[0].plot(history.history['val_loss'], 'r', label=r'$\alpha = 0.1$') 
  elif x == 0.001:
    ax[0].plot(history.history['val_loss'], 'g', label=r'$\alpha = 0.001$') 
  elif x == 0.0001:
    ax[0].plot(history.history['val_loss'], 'c', label=r'$\alpha = 0.0001$')
  else: 
    ax[0].plot(history.history['val_loss'], 'm', label=r'$\alpha = 0.000001$')
    
ax[0].set_xlabel('Epochs')
ax[0].set_ylabel('Loss')
ax[0].legend();
#print(history.history.keys())
#history.history['val_acc']

ax[1].set_title("Validation Accuracy")

for x in learning_rate:
  model = logistic_regression(num_inputs=2, regularization=0, learning_rate= x)
  history = model.fit(X_train[:, [0,1]], y_train, epochs=50, batch_size=32, validation_split=0.2)
  if x == 1.0:
    ax[1].plot(history.history['val_acc'], 'b', label=r'$\alpha = 1.0$')  
  elif x == 0.1:
    ax[1].plot(history.history['val_acc'], 'r', label=r'$\alpha = 0.1$') 
  elif x == 0.001:
    ax[1].plot(history.history['val_acc'], 'g', label=r'$\alpha = 0.001$') 
  elif x == 0.0001:
    ax[1].plot(history.history['val_acc'], 'c', label=r'$\alpha = 0.0001$')
  else: 
    ax[1].plot(history.history['val_acc'], 'm', label=r'$\alpha = 0.000001$')
    
ax[1].set_xlabel('Epochs')
ax[1].set_ylabel('Loss')
ax[1].legend();

# Choose the best learning rate from the previous experiment

# Train 5 logistic regression models on this data with the following values for lambda: 1.0, 0.1, 0.001, 0.0001, 0.000001
# Plot your learning curve for each model (train and validation loss together, so 2 curves on each plot)
# Use a batch size of 32
# Train for 50 epochs
# Modify the code IN THIS CELL for this task


# from the previous plots we will select learning rate alpha to be 0.1

f, ax = plt.subplots(1, 5, figsize=(25, 5))
lambda_values = [1.0, 0.1, 0.001, 0.0001, 0.000001]
alpha = 0.1
for L in lambda_values:
  model = logistic_regression(num_inputs=2, regularization=L, learning_rate= alpha)
  history = model.fit(X_train[:, [0,1]], y_train, epochs=50, batch_size=32, validation_split=0.2)
  if L == 1.0:
    ax[0].plot(history.history['loss'], 'b', label='train loss')
    ax[0].plot(history.history['val_loss'], 'g', label='validation loss')
    ax[0].set_title('$\lambda$ = 1.0')
    ax[0].set_xlabel('Epochs')
    ax[0].set_ylabel('Loss')
    ax[0].legend();
  elif L == 0.1:
    ax[1].plot(history.history['loss'], 'b', label='train loss')
    ax[1].plot(history.history['val_loss'], 'g', label='validation loss') 
    ax[1].set_title('$\lambda$ = 0.1')
    ax[1].set_xlabel('Epochs')
    ax[1].set_ylabel('Loss')
    ax[1].legend();
  elif L == 0.001:
    ax[2].plot(history.history['loss'], 'b', label='train loss')
    ax[2].plot(history.history['val_loss'], 'g', label='validation loss')
    ax[2].set_title('$\lambda$ = 0.001')
    ax[2].set_xlabel('Epochs')
    ax[2].set_ylabel('Loss')
    ax[2].legend();
  elif L == 0.0001:
    ax[3].plot(history.history['loss'], 'b', label='train loss')
    ax[3].plot(history.history['val_loss'], 'g', label='validation loss')
    ax[3].set_title('$\lambda$ = 0.0001')
    ax[3].set_xlabel('Epochs')
    ax[3].set_ylabel('Loss')
    ax[3].legend();
  else: 
    ax[4].plot(history.history['loss'], 'b', label='train loss')
    ax[4].plot(history.history['val_loss'], 'g', label='validation loss')
    ax[4].set_title('$\lambda$ = 0.000001')
    ax[4].set_xlabel('Epochs')
    ax[4].set_ylabel('Loss')
    ax[4].legend();